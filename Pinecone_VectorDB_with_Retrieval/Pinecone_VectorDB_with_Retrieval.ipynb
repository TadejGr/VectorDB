{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "05b0ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f13302",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db95a990",
   "metadata": {},
   "source": [
    "# Convert PDF text to chunks of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b522715",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFDirectoryLoader('pdfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51aab9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de9bdb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1260666",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks=text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b13fe2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗ †\n",
      "University of Toronto\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66b9693b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e37dea",
   "metadata": {},
   "source": [
    "# Connect to Pinecone vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c9afb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09b17bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6da24b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf15783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7cd6a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "db0462c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0929e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name=\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "206f3ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010882d1",
   "metadata": {},
   "source": [
    "# Insert vectors into Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "11553a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(index_name=index, embedding=embedding)\n",
    "    \n",
    "batch_size = 10\n",
    "for i in range(0, len(text_chunks), batch_size):\n",
    "    chunk_batch = text_chunks[i:i + batch_size]\n",
    "    vector_store.add_documents(chunk_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02ce865",
   "metadata": {},
   "source": [
    "# Perform similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7dc83af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].', metadata={'page': 1.0, 'source': 'pdfs\\\\transformer.pdf'}),\n",
       " Document(page_content='during training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.', metadata={'page': 5.0, 'source': 'pdfs\\\\transformer.pdf'}),\n",
       " Document(page_content='executed operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', metadata={'page': 5.0, 'source': 'pdfs\\\\transformer.pdf'}),\n",
       " Document(page_content='End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-', metadata={'page': 1.0, 'source': 'pdfs\\\\transformer.pdf'})]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"What is self attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "382d383b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index.delete(delete_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5842b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0301c",
   "metadata": {},
   "source": [
    "# Getting an answer using Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "373a3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "29ef21fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"what are End-to-end memory networks based on?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9a94933a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what are End-to-end memory networks based on?',\n",
       " 'result': ' End-to-end memory networks are based on a recurrent attention mechanism.'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
